{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Python3.5.2（Anaconda）开发，TensorFlow版本是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的配置\n",
    "\n",
    "方便起见，在这里定义神经网络的配置，你可以很容易找到或改变这些数值，然后重新运行Notebook。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST数据集大约12MB，如果没在文件夹中找到就会自动下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainLabels is y_train\n",
    "train[\"label\"] = train[\"label\"].astype('int32')\n",
    "trainLabels = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "# train pixels is x_train\n",
    "trainPixels = train.drop(labels = [\"label\"],axis = 1)\n",
    "# free some space\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainPixels = trainPixels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as ses:\n",
    "    NUM_CLASSES = 10\n",
    "\n",
    "    labels = trainLabels\n",
    "    onehot_labels = tf.one_hot(labels, depth=10, on_value=None, off_value=None, axis=None, dtype=None, name=None).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.countplot(trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的源码中，有很多地方用到了数据维度。它们只在一个地方定义，因此我们可以在代码中使用这些数字而不是直接写数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数用来根据给定大小创建TensorFlow变量，并将它们用随机值初始化。需注意的是在此时并未完成初始化工作，仅仅是在TensorFlow图里定义它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积层的帮助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数为TensorFlow在计算图里创建了新的卷积层。这里并没有执行什么计算，只是在TensorFlow图里添加了数学公式。\n",
    "\n",
    "假设输入的是四维的张量，各个维度如下：\n",
    "\n",
    "1. 图像数量\n",
    "2. 每张图像的Y轴\n",
    "3. 每张图像的X轴\n",
    "4. 每张图像的通道数\n",
    "\n",
    "输入通道可能是彩色通道，当输入是前面的卷积层生成的时候，它也可能是滤波通道。\n",
    "\n",
    "输出是另外一个4通道的张量，如下：\n",
    "\n",
    "1. 图像数量，与输入相同\n",
    "2. 每张图像的Y轴。如果用到了2x2的池化，是输入图像宽高的一半。\n",
    "3. 每张图像的X轴。同上。\n",
    "4. 卷积滤波生成的通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换一个层的帮助函数\n",
    "\n",
    "卷积层生成了4维的张量。我们会在卷积层之后添加一个全连接层，因此我们需要将这个4维的张量转换成可被全连接层使用的2维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个全连接层的帮助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数为TensorFlow在计算图中创建了一个全连接层。这里也不进行任何计算，只是往TensorFlow图中添加数学公式。\n",
    "\n",
    "输入是大小为`[num_images, num_inputs]`的二维张量。输出是大小为`[num_images, num_outputs]`的2维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 占位符 （Placeholder）变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder是作为图的输入，每次我们运行图的时候都可能会改变它们。将这个过程称为feeding placeholder变量，后面将会描述它。\n",
    "\n",
    "首先我们为输入图像定义placeholder变量。这让我们可以改变输入到TensorFlow图中的图像。这也是一个张量（tensor），代表一个多维向量或矩阵。数据类型设置为float32，形状设为`[None, img_size_flat]`，`None`代表tensor可能保存着任意数量的图像，每张图象是一个长度为`img_size_flat`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层希望`x`被编码为4维张量，因此我们需要将它的形状转换至`[num_images, img_height, img_width, num_channels]`。注意`img_height == img_width == img_size`，如果第一维的大小设为-1， `num_images`的大小也会被自动推导出来。转换运算如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们为输入变量`x`中的图像所对应的真实标签定义placeholder变量。变量的形状是`[None, num_classes]`，这代表着它保存了任意数量的标签，每个标签是长度为`num_classes`的向量，本例中长度为10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以为class-number提供一个placeholder，但这里用argmax来计算它。这里只是TensorFlow中的一些操作，没有执行什么运算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.argmax(array, dimension=1) = np.argmax(array, axis=1)<p>\n",
    "如果array是多维数组，dimemsion或axis=1的意思就是取每个小数组中的最大值组成新的数组<p>\n",
    "axis=0: 取同一位置下有最大值的数组序号，遍历每个位置得到新的数组<p>\n",
    "简单的说，axis指定了在矩阵的哪一个维度上进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层 1\n",
    "\n",
    "创建第一个卷积层。将`x_image`当作输入，创建`num_filters1`个不同的滤波器，每个滤波器的宽高都与 `filter_size1`相等。最终我们会用2x2的max-pooling将图像降采样，使它的尺寸减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查卷积层输出张量的大小。它是（？,14, 14, 16），这代表着有任意数量的图像（？代表数量），每张图像有14个像素的宽和高，有16个不同的通道，每个滤波器各有一个通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(?, 14, 14, 16) dtype=float32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层 2\n",
    "\n",
    "创建第二个卷积层，它将第一个卷积层的输出作为输入。输入通道的数量对应着第一个卷积层的滤波数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核对一下这个卷积层输出张量的大小。它的大小是（？， 7， 7， 36）,其中？也代表着任意数量的图像，每张图有7像素的宽高，每个滤波器有36个通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(?, 7, 7, 36) dtype=float32>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换层\n",
    "\n",
    "这个卷积层输出一个4维张量。现在我们想将它作为一个全连接网络的输入，这就需要将它转换成2维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个张量的大小是（？， 1764），意味着共有一定数量的图像，每张图像被转换成长为1764的向量。其中1764 = 7 x 7 x 36。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(?, 1764) dtype=float32>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层 1\n",
    "\n",
    "往网络中添加一个全连接层。输入是一个前面卷积得到的被转换过的层。全连接层中的神经元或节点数为`fc_size`。我们可以用ReLU来学习非线性关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层的输出是一个大小为（？，128）的张量，？代表着一定数量的图像，并且`fc_size` == 128。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_5:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层 2\n",
    "\n",
    "添加另外一个全连接层，它的输出是一个长度为10的向量，它确定了输入图是属于哪个类别。这层并没有用到ReLU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_7:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个全连接层估算了输入图有多大的可能属于10个类别中的其中一个。然而，这是很粗略的估计并且很难解释，因为数值可能很小或很大，因此我们会对它们做归一化，将每个元素限制在0到1之间，并且相加为1。这用一个称为softmax的函数来计算的，结果保存在`y_pred`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类别数字是最大元素的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使模型更好地对输入图像进行分类，我们必须改变`weights`和`biases`变量。首先我们需要对比模型`y_pred`的预测输出和期望输出的`y_true`，来了解目前模型的性能如何。\n",
    "\n",
    "交叉熵（cross-entropy）是在分类中使用的性能度量。交叉熵是一个常为正值的连续函数，如果模型的预测值精准地符合期望的输出，它就等于零。因此，优化的目的就是通过改变网络层的变量来最小化交叉熵。\n",
    "\n",
    "TensorFlow有一个内置的计算交叉熵的函数。这个函数内部计算了softmax，所以我们要用`layer_fc2`的输出而非直接用`y_pred`,因为`y_pred`上已经计算了softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们为每个图像分类计算了交叉熵，所以有一个当前模型在每张图上表现的度量。但是为了用交叉熵来指导模型变量的优化，我们需要一个额外的标量值，因此简单地利用所有图像分类交叉熵的均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然我们有一个需要被最小化的损失度量，接着就可以建立优化一个优化器。这个例子中，我们使用的是梯度下降的变体`AdamOptimizer`。\n",
    "\n",
    "优化过程并不是在这里执行。实际上，还没计算任何东西，我们只是往TensorFlow图中添加了优化器，以便之后的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要另外一些性能度量，来向用户展示这个过程。\n",
    "\n",
    "这是一个布尔值向量，代表预测类型是否等于每张图片的真实类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的计算先将布尔值向量类型转换成浮点型向量，这样子False就变成0，True变成1，然后计算这些值的平均数，以此来计算分类的准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建TensorFlow会话（session）\n",
    "\n",
    "一旦创建了TensorFlow图，我们需要创建一个TensorFlow会话，用来运行图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化变量\n",
    "\n",
    "我们需要在开始优化weights和biases变量之前对它们进行初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用来优化迭代的帮助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集中有50,000张图。用这些图像计算模型的梯度会花很多时间。因此我们利用随机梯度下降的方法，它在优化器的每次迭代里只用到了一小部分的图像。\n",
    "\n",
    "如果内存耗尽导致电脑死机或变得很慢，你应该试着减少这些数量，但同时可能还需要更优化的迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if the data is correct\n",
    "inputPic = trainPixels[3].reshape(28,28)\n",
    "plt.imshow(inputPic, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputPic = trainPixels[4].reshape(28,28)\n",
    "plt.imshow(inputPic, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数执行了多次的优化迭代来逐步地提升网络层的变量。在每次迭代中，从训练集中选择一批新的数据，然后TensorFlow用这些训练样本来执行优化器。每100次迭代会打印出相关信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 256\n",
    "# trainPixels = trainPixels.values\n",
    "# print(trainPixels.values)\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "\n",
    "        index = random.randint(0,41936)\n",
    "        x_batch = trainPixels[index:index + 64]\n",
    "        y_true_batch = onehot_labels[index:index + 64]\n",
    "        \n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,000次优化迭代后的性能\n",
    "\n",
    "经过10,000次优化迭代后，测试集上的分类准确率高达99%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    301, Training Accuracy:  67.2%\n",
      "Optimization Iteration:    401, Training Accuracy:  82.8%\n",
      "Optimization Iteration:    501, Training Accuracy:  90.6%\n",
      "Optimization Iteration:    601, Training Accuracy:  95.3%\n",
      "Optimization Iteration:    701, Training Accuracy:  95.3%\n",
      "Optimization Iteration:    801, Training Accuracy:  95.3%\n",
      "Optimization Iteration:    901, Training Accuracy:  95.3%\n",
      "Optimization Iteration:   1001, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   1101, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   1201, Training Accuracy:  93.8%\n",
      "Optimization Iteration:   1301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   1401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   1501, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   1601, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   1701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   1801, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   1901, Training Accuracy:  95.3%\n",
      "Optimization Iteration:   2001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2101, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   2201, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   2301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2401, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   2501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2801, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   2901, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   3001, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   3101, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   3201, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   3301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   3401, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   3501, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   3601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   3701, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   3801, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   3901, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   4001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4101, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   4201, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4801, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   4901, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5101, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5201, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   5301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   5801, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   5901, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6101, Training Accuracy:  96.9%\n",
      "Optimization Iteration:   6201, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   6301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6801, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   6901, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7101, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7201, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7801, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   7901, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8001, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8101, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8201, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8301, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8401, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8501, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8601, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8701, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   8801, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   8901, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   9001, Training Accuracy:  98.4%\n",
      "Optimization Iteration:   9101, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   9201, Training Accuracy: 100.0%\n",
      "Time usage: 0:00:57\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=9000) # We performed 1000 iterations above. [9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputPic = trainPixels[0].reshape(28,28)\n",
    "plt.imshow(inputPic, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argmaxtensor = tf.argmax(y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# session.run(tf.global_variables_initializer())\n",
    "# softmax_tensor = session.graph.get_tensor_by_name('final_result:0')\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "print(test.shape)\n",
    "test.head()\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADilJREFUeJzt3X+MVPW5x/HPI9JggD9AVkGLLhfNzTXGUjPBGkW9VCvckGCjbIpJQ7V0iynRJjW5Bv+oxpiQGwsXzU3N9ropJC1QBZUYvELIJZZoGgd/VCjeW2O2FJfAItVSNVlln/vHHnpX3PnOMHNmzizP+5WQnTnPOXuenPDZMzPfM+dr7i4A8ZxTdAMAikH4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EdW4rdzZt2jTv7Oxs5S6BUPr6+nTs2DGrZd2Gwm9mCyStkzRO0n+6++rU+p2dnSqXy43sEkBCqVSqed26X/ab2ThJ/yFpoaQrJC01syvq/X0AWquR9/xzJb3r7u+5+6CkTZIW59MWgGZrJPwXS/rziOeHsmVfYGbdZlY2s/LAwEADuwOQp0bCP9qHCl/6frC797h7yd1LHR0dDewOQJ4aCf8hSTNHPP+qpP7G2gHQKo2E/zVJl5vZLDP7iqTvSNqWT1sAmq3uoT53/9zMVkp6ScNDfb3uvj+3zgA0VUPj/O6+XdL2nHoB0EJc3gsERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQDc3Sa2Z9kk5IOinpc3cv5dEUgOZrKPyZf3b3Yzn8HgAtxMt+IKhGw++SdpjZXjPrzqMhAK3R6Mv+69y938wukLTTzN5x95dHrpD9UeiWpEsuuaTB3QHIS0Nnfnfvz34elfSspLmjrNPj7iV3L3V0dDSyOwA5qjv8ZjbRzCafeizpW5L25dUYgOZq5GX/hZKeNbNTv+fX7v5fuXQFoOnqDr+7vyfpazn2ggIMDg4m6xs2bEjW16xZk6wfOHCgYu28885Lbvvpp58m6/fcc0+y/uijj1asTZkyJbltBAz1AUERfiAowg8ERfiBoAg/EBThB4LK41t9aGP9/f3J+vLly5P1vXv3JuuPPPJIsn7DDTdUrE2cODG57Ysvvpis7969O1nv6uqqWNu5c2dy2wg48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzznwVeffXVirVFixYlt73mmmuS9VdeeSVZnz17drLeiO7u9G0h77zzzmR93rx5FWsnTpxIbjt58uRk/WzAmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcfwx4//33k/UlS5ZUrF199dXJbbdv315XT+3gmWeeSdaPHz9esXbuufzX58wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0FVHew0s15JiyQddfcrs2VTJW2W1CmpT1KXu/+leW3Gdv/99yfrJ0+erFjbvHlz3u20TLVrEO69995kffXq1RVr1aYHj6CWM/8vJS04bdkDkna5++WSdmXPAYwhVcPv7i9LOv1SqcWS1meP10u6Lee+ADRZve/5L3T3w5KU/bwgv5YAtELTP/Azs24zK5tZeWBgoNm7A1CjesN/xMxmSFL282ilFd29x91L7l7q6Oioc3cA8lZv+LdJWpY9Xibp+XzaAdAqVcNvZhslvSrpH83skJl9X9JqSbeY2R8l3ZI9BzCGVB3nd/elFUrfzLmXsLZs2ZKsP/fcc8l6aq75qVOn1tVTK+zatStZv/vuu5P1xx57LFmvdt//6LjCDwiK8ANBEX4gKMIPBEX4gaAIPxAU9y9uA3v37k3WL7vssmT9+uuvz7OdM/Lhhx8m6+vWratY6+3tTW47f/78ZL3aUCDSOPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM848Bg4ODdW/70UcfJev79+9P1qt93fiNN95I1idMmFCxNjQ0lNx2zZo1yTrTbDeGMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMVAaRu48cYbk/W1a9cm65deemnF2ieffJLc9oMPPkjWFyw4fYLmL3r44YeT9a6uroq1xx9/PLnt9OnTk3U0hjM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVdZzfzHolLZJ01N2vzJY9JOkHkgay1Va5+/ZmNXm2u/XWW5P1rVu3Jus7duyoWKs2Vj5v3rxk/dprr03WV6xYkayff/75FWu33357cls0Vy1n/l9KGu1Kj7XuPif7R/CBMaZq+N39ZUnHW9ALgBZq5D3/SjP7vZn1mtmU3DoC0BL1hv/nkmZLmiPpsKSfVVrRzLrNrGxm5YGBgUqrAWixusLv7kfc/aS7D0n6haS5iXV73L3k7qWOjo56+wSQs7rCb2YzRjz9tqR9+bQDoFVqGerbKOkmSdPM7JCkn0q6yczmSHJJfZJ+2MQeATRB1fC7+9JRFj/VhF5QwcKFCxuqN2LTpk3Jek9PT7KeugbhnHO4xqxIHH0gKMIPBEX4gaAIPxAU4QeCIvxAUNy6O7h33nknWV++fHmyftdddyXrN9988xn3hNbgzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHOf5YbHBxM1u+4445kfdasWcl6tWm2zSxZR3E48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzn+U2btyYrPf39yfru3fvTtYnTZp0pi2hTXDmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgqo7zm9lMSRskTZc0JKnH3deZ2VRJmyV1SuqT1OXuf2leq6hk3759FWsrV65MbtvV1ZWsX3XVVXX1hPZXy5n/c0k/cfd/kvQNST8ysyskPSBpl7tfLmlX9hzAGFE1/O5+2N1fzx6fkHRA0sWSFktan622XtJtzWoSQP7O6D2/mXVK+rqk30m60N0PS8N/ICRdkHdzAJqn5vCb2SRJWyT92N3/egbbdZtZ2czKAwMD9fQIoAlqCr+Zjddw8H/l7luzxUfMbEZWnyHp6GjbunuPu5fcvdTR0ZFHzwByUDX8Nnz71ackHXD3NSNK2yQtyx4vk/R8/u0BaJZavtJ7naTvSnrbzN7Mlq2StFrSb8zs+5IOSlrSnBbx8ccfJ+up4bqZM2cmt33yySfr6qkdVLst+dDQUMXahAkT8m5nzKkafnffI6nSzde/mW87AFqFK/yAoAg/EBThB4Ii/EBQhB8IivADQXHr7jFgxYoVyfrBgwcr1t56663ktuPHj6+rp3bw4IMPJus7d+6sWLvvvvuS2y5Zkr5s5Wy4ZTlnfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+NrBnz55k/emnn07We3t7K9Zmz55dV09jQbVx/nHjxlWsvfDCC8ltX3rppWR906ZNyfpYwJkfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Iyd2/ZzkqlkpfL5Zbtr1189tlnyfr8+fOT9Tlz5iTrTzzxxBn3hLNTqVRSuVyudKv9L+DMDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBVf0+v5nNlLRB0nRJQ5J63H2dmT0k6QeSBrJVV7n79mY1OpYNDAwk6/v370/We3p68mwHkFTbzTw+l/QTd3/dzCZL2mtmp2ZDWOvujzWvPQDNUjX87n5Y0uHs8QkzOyDp4mY3BqC5zug9v5l1Svq6pN9li1aa2e/NrNfMplTYptvMymZWrvbyF0Dr1Bx+M5skaYukH7v7XyX9XNJsSXM0/MrgZ6Nt5+497l5y91JHR0cOLQPIQ03hN7PxGg7+r9x9qyS5+xF3P+nuQ5J+IWlu89oEkLeq4Tczk/SUpAPuvmbE8hkjVvu2pH35twegWWr5tP86Sd+V9LaZvZktWyVpqZnNkeSS+iT9sCkdngUuuuiiZP348eMt6gT4f7V82r9H0mjfD2ZMHxjDuMIPCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVEun6DazAUl/GrFomqRjLWvgzLRrb+3al0Rv9cqzt0vdvab75bU0/F/auVnZ3UuFNZDQrr21a18SvdWrqN542Q8ERfiBoIoOfzvPQ9WuvbVrXxK91auQ3gp9zw+gOEWf+QEUpJDwm9kCM/sfM3vXzB4ooodKzKzPzN42szfNrFxwL71mdtTM9o1YNtXMdprZH7Ofo06TVlBvD5nZ+9mxe9PM/qWg3maa2X+b2QEz229m92XLCz12ib4KOW4tf9lvZuMk/a+kWyQdkvSapKXu/oeWNlKBmfVJKrl74WPCZnaDpL9J2uDuV2bL/k3ScXdfnf3hnOLu/9omvT0k6W9Fz9ycTSgzY+TM0pJuk/Q9FXjsEn11qYDjVsSZf66kd939PXcflLRJ0uIC+mh77v6ypNNn9FgsaX32eL2G//O0XIXe2oK7H3b317PHJySdmlm60GOX6KsQRYT/Ykl/HvH8kNprym+XtMPM9ppZd9HNjOLCbNr0U9OnX1BwP6erOnNzK502s3TbHLt6ZrzOWxHhH232n3YacrjO3a+WtFDSj7KXt6hNTTM3t8ooM0u3hXpnvM5bEeE/JGnmiOdfldRfQB+jcvf+7OdRSc+q/WYfPnJqktTs59GC+/m7dpq5ebSZpdUGx66dZrwuIvyvSbrczGaZ2VckfUfStgL6+BIzm5h9ECMzmyjpW2q/2Ye3SVqWPV4m6fkCe/mCdpm5udLM0ir42LXbjNeFXOSTDWX8u6Rxknrd/dGWNzEKM/sHDZ/tpeFJTH9dZG9mtlHSTRr+1tcRST+V9Jyk30i6RNJBSUvcveUfvFXo7SYNv3T9+8zNp95jt7i36yX9VtLbkoayxas0/P66sGOX6GupCjhuXOEHBMUVfkBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgvo/Ygv9SLjKP6YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputPic = test[2].reshape(28,28)\n",
    "plt.imshow(inputPic, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {x: test[2].reshape(1,784)}\n",
    "y_pred = session.run(argmaxtensor, feed_dict=feed_dict)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# answers = []\n",
    "\n",
    "# for img in test:\n",
    "#     feed_dict = {x: img.reshape(1,784)}\n",
    "#     y_pred = session.run(argmaxtensor, feed_dict=feed_dict)\n",
    "#     answers.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answers[27999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经用TensorFlow完成了任务，关闭session，释放资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This has been commented out in case you want to modify and experiment\n",
    "# with the Notebook without having to restart it.\n",
    "# session.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "575px",
    "left": "1009px",
    "right": "20px",
    "top": "108px",
    "width": "510px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
